\documentclass[format=sigplan,review=false,screen=true]{acmart}\settopmatter{}

\usepackage[para]{footmisc}   %% gather footnotes on a single line

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{supertabular}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{multirow}
\usepackage{calc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{dashrule}  % for \hdashrule
\usepackage{stackrel}
\usepackage{enumerate}
\usepackage{framed}
\usepackage{bussproofs}
\usepackage{mdframed}  % highlight
\usepackage{hyperref}
\usepackage{comment}
%\usepackage{natbib}
\usepackage{ifthen}
\usepackage{pdftexcmds}

\usepackage{mathpartir}
\usepackage{ottalt}

\usepackage{fancyvrb}

\usepackage{listings}
\lstset{language=Haskell}

%\special{papersize=8.5in,11in}
%\setlength{\pdfpageheight}{\paperheight}
%\setlength{\pdfpagewidth}{\paperwidth}

%\usepackage{palatino}
\renewcommand{\familydefault}{\rmdefault}
%\renewcommand{\ttdefault}{cmtt}

%% Show admissible premises in rules
%% This should be false in main body of text and true in the appendix.
\newif\ifadmissible
\admissiblefalse
\newcommand\suppress[1]{\ifadmissible{[#1]}\else{}\fi}
\inputott{ett-rules}


\newcommand{\alt}{\ |\ }
\newcommand{\rul}[1]{\rref{#1}}


\newcommand{\fc}{DC\xspace}
\newcommand{\fimp}{D\xspace}
\newcommand{\pico}{\textsc{PiCo}\xspace}


\newif\ifcomments
\commentstrue
\ifcomments
\newcommand{\scw}[1]{\textcolor{blue}{{SCW: #1}}}
\newcommand{\rae}[1]{\textcolor{magenta}{{RAE: #1}}}
\newcommand\av[1]{\textcolor{orange}{{AV: #1}}}
\else
\newcommand{\scw}[1]{}
\newcommand{\rae}[1]{}
\newcommand\av[1]{}
\fi




%% allow more interline spacing (and fewer overfull hboxes).
\tolerance=5000

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission
%\setcopyright{none}             %% For review submission
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


\title{A specification of eta-equivalence in LNrep}

\author{Stephanie Weirich}
\orcid{0000-0002-6756-9168}
\affiliation{
  \position{Professor}
  \department{Computer and Information Science}              %% \department is recommended
  \institution{University of Pennsylvania}            %% \institution is required
  \streetaddress{3330 Walnut St}
  \city{Philadelphia}
  \state{PA}
  \postcode{19104}
  \country{USA}
}
\email{sweirich@cis.upenn.edu}

\author{Antoine Voizard}
\affiliation{
  \position{}
  \department{}              %% \department is recommended
  \institution{University of Pennsylvania}            %% \institution is required
  \streetaddress{}
  \city{}
  \state{}
  \postcode{}
  \country{USA}
}
\email{voizard@seas.upenn.edu}

\begin{abstract}
abstract
\end{abstract}

\maketitle

%% Our running head is too long for acmsmall
\renewcommand{\shortauthors}{Weirich, Voizard}

Disclaimer: the current prose will need a loooot of refinement/improvement

\section{Introduction}

The Corespec project is a large (large?) scale mechanization in Coq of the metatheory of System D and System DC,
two related, dependently-typed languages aimed at replacing GHC's internal language, Core. In this talk, we propose to
take a look at this development through the lens of a recent addition to it, some of the $\eta$-reduction rules. In
particular, this will allow us to explore (better word?) how the locally nameless variable-binding representation
works in practice for mechanized metatheory.

\section{Corespec}
The idea behind Corespec is to provide a replacement for GHC Core that supports full spectrum dependent types.
In particular, this requires untangling the notions of type on the one hand and of erasable component (computationally irrelevant)
on the other - which current GHC essentially conflates.

The problem with such design is that the resulting language(s), combining several important features (dependent types, irrelevance,
explicit type coercions, non-termination) is/are quite complex, with a design tricky to get precisely right. This is the main reason why
mechanizing our formalization in Coq has been invaluable in getting strong confidence in our results, but also in designing the system
in the first place. TODO: should say we designed directly in Coq, not
mechanized afterwards
\scw{That is only sort-of true. There were many many papers about FC that we
  drew from. Even though this was a differnt system, and mechanization lead to
new designs, such as AnRefl2, we weren't working from scratch.}

Potential ideas: the "breaking the design + recompiling" methodology

\scw{We need to explain much more precisely about what definitions are
  provided by Ott, what lemmas are provided by
  lngen, and what we actually proved.  maybe pick some representative
  lemmas such as confluence to state?}

\subsection{Locally Nameless representation}
Used a LN repr, With Ott and Lngen, Ott: Coq definitions from high level specification, Lngen: deriving maaany helper lemmas about the representation


How do we specify eta-reduction using the locally nameless representation?

As part of a larger proof development, we added eta reduction using the
following rule:

...

\scw{This rule captures the free variable condition because it allows x to
  appear in a (because a is under the binder \\x) but x cannot appear in b
(because b doesn't appear under the binder).}

\begin{verbatim}
 |- b => b'
a = b x
----------------------------- :: Eta
 |- \ x. a => b'
\end{verbatim}

\scw{Here is the rule that we might imagine writing in Ott. Sadly, it is
  outside the scope of what the LN backend for Ott can handle.  }

\begin{verbatim}
 |- b => b'
x `notin` fv b
----------------------------- :: EtaPaper?
 |- \ x. b x => b'
\end{verbatim}


\begin{verbatim}
Induction exp :=
   | var_f : atom -> exp
   | var_b : nat -> exp
   | abs   : exp -> exp
   | app   : exp -> exp -> exp
\end{verbatim}

The rule we used (cofinite quantification)
\begin{verbatim}
Par_Eta : forall a b b',
  Par b b' ->
  (forall x, x notin L -> open a x = app b x) ->
  Par (abs a) b'
\end{verbatim}

Alternative exists fresh
\begin{verbatim}
Par_Eta : forall a b b' x,
  x notin ???
  Par b b' ->
  open a x = app b x ->
  Par (abs a) b'
\end{verbatim}

Alternative close version
\begin{verbatim}
Par_Eta : forall a b x,
  Par a b ->
  Par (abs (close x (app a (var_f x)))) b
\end{verbatim}

TODO: explain why they work



\section{Lessons learned}
\scw{These lessons are far too general for coqpl.  that group already believes in
  the benefit of mechanization, so won't be impressed by general statements
  about the usefulness of coq for pl. }
- With some infrastructure, it can actually be quite quick to mechanize (not an absurdly longer time than on paper), which makes it quite useful provided the confidence

- Mecha (and coq in particular) becomes a critical enabler in the study of
such complex, intricate, but interesting languages. Esp. for a compiler,
having a internal lang with a mechanized metatheory is quite cool


\scw{Here's how to make it concrete: we can mention the inversion lemma that
  doesn't hold. Not only is this something that surprised me, but it is also
  good to know that we don't need it to hold.}
\begin{lemma}[Inversion]
if $[[G |= \x.b x : Pi x : A.B ]]$ and $[[ x `notin` fv b]]$ then $[[ G |= b :
Pi x:A.B ]]$.
\end{lemma}
The lemma fails because the erased language only mentions variables in
computationally relevant positions. However, we could have used x implicitly,
such as in instantiating a polymorphic function.

Because this lemma doesn't hold, our parallel reduction relation does not have
the preservation property. The rule above allows the term to reduce, in some
cases to an ill-typed term.  However, none of the rest of the metatheory
relies on preservation for parallel reduction---and we can verify that easily
with the proof scripts.

- Collaboration benefit: we were able to bring in a new collaborator (Ana)
after all of the proofs were done, and she could make progress without having
to understand the entire development. \scw{Ana, can you say anything specific
about joining this scale of project?}


\end{document}




%% Local Variables:
%% mode: LaTeX
%% End:

%%  LocalWords:  interline overfull hboxes papersize FL Ahmed eir sweirich HM
%%  LocalWords:  Hamidhasan taun SCW lncs urgh app rccll damas milner HMV SB
%%  LocalWords:  Damas's outsidein SB's ghc Expr normalizePoly normalizeProxy
%%  LocalWords:  normalizeExpr TypeApplications AllowAmbiguousTypes APIs API
%%  LocalWords:  ICFP Refl SCond MonadReader MonadWriter RAE polytype const
%%  LocalWords:  NB SwapPair swapPair sP HM's monotypes Barendregt Gen InstG
%%  LocalWords:  monotype vars InstS hmv Abs TApp Var V's Annot algv foo DAbs
%%  LocalWords:  RankNTypes metatheorems skolemization inst'd DeepSkol Skol
%%  LocalWords:  Twelf Dreyer Blume's ML's DK Neel Krishnaswami Didier hlio
%%  LocalWords:  TypeOperators DataKinds PolyKinds ConstraintKinds Typeable
%%  LocalWords:  ScopedTypeVariables woozle boolCast eqT unsafeThe Val Cond
%%  LocalWords:  eval SExpr sEval SBool SVal sIf Inst IFPOP ListInst infixr
%%  LocalWords:  STrue SFalse supertype checkIf myId myAbs abs Num GHCi ghci
%%  LocalWords:  fromInteger myPair MkG pr Ty forall ol pid cc hm sp inst gen
%%  LocalWords:  sb dn pf sf ys xs se
